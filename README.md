# Парсер документации Python
### Как установить проект:
Клонируем репозеторий с GitHub:
```
git clone https://github.com/ggerasyanov/scrapy_parser_pep.git
```
Перейти в корневую папку проекта:
```
cd .../scrapy_parser_pep/
```
Создать и активировать виртуальное окружение:
```
python -m venv venv
```
```
source venv/Scripts/activate
```
Обновить менеджер пакетов pip:
```
python -m pip install --upgrade pip
```
Установить зависимости из файла requirements.txt:
```
pip install -r requirements.txt
```
Парсер готов к работе.

### Возможности парсера:
Парсер собирает информацию по документам PEP — статус, номер документа и название. Так же считает количество документов в конкретных статусах.

### Как запустить парсер:
```
#.../scrapy_parser_pep/
scrapy crawl pep
```

### Вывод данных:
Парсер сохраняет информацию в два файла. Они лежат в папке .../scrapy_parser_pep/results/.

Первый файл "pep_ДатаВремя.csv" содержит все документы PEP их номера, названия и статусы.

Второй файл "status_summary_ДатаВремя.csv" содержит количество документов в разных статусах и общее количетсво документов.
